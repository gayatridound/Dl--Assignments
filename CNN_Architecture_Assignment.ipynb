{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93aa3a9-07e0-41ab-801a-bcd375c555dc",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d94a9fe0-05c3-4db5-8eda-f6e8e15aa26e",
   "metadata": {},
   "source": [
    "1.Describe the purpose and benifitas of pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b3aba-fbc7-429f-a1fa-41cc38e8d935",
   "metadata": {},
   "source": [
    "Pooling layers are a key component in Convolutional Neural Networks (CNNs) that serve several important purposes:\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "One of the main benefits of pooling is that it reduces the spatial dimensions (width and height) of the feature maps generated by convolutional layers. This downsampling helps to:\n",
    "Reduce the number of parameters: Fewer parameters lead to a less complex model, which requires less memory and can be trained faster.\n",
    "\n",
    "Translation Invariance:\n",
    "\n",
    "Pooling layers also contribute to a property called translation invariance. This means that the network becomes less sensitive to small shifts in the position of an object within the image. By summarizing information from a local region, pooling allows the network to recognize the presence of a feature regardless of its exact location.\n",
    "\n",
    "Feature Extraction:\n",
    "\n",
    "In a broader sense, pooling can be seen as a form of feature extraction. Different pooling operations (like max pooling and average pooling) capture distinct aspects of the features learned by the convolutional layers.\n",
    "    Max pooling emphasizes the most prominent features, often edges and corners, which can be useful for object recognition.\n",
    "    Average pooling provides a smoother representation that retains more spatial information, making it suitable for tasks like image segmentation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7758a99-dd80-466e-9867-9d262fc82f7e",
   "metadata": {},
   "source": [
    "2.Explain the differance between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756277-3d0f-41ee-ade0-5660eb82588f",
   "metadata": {},
   "source": [
    "Max Pooling:\n",
    "\n",
    "Max pooling, the more common choice, focuses on the most prominent features within a local region of the feature map.\n",
    "During max pooling, a filter slides across the feature map, and for each region covered by the filter, the maximum element is selected as the output value.\n",
    "This operation is particularly useful for tasks like object recognition where edges, corners, and high-contrast regions hold significant information.\n",
    "In essence, max pooling emphasizes the presence of a feature even if its exact location varies slightly.\n",
    "\n",
    "Min Pooling:\n",
    "\n",
    "Min pooling, on the other hand, captures the least prominent features within a local region.\n",
    "It selects the minimum element from each region covered by the filter in the feature map.\n",
    "While less frequently used, min pooling can be helpful in specific scenarios. For example, if you're dealing with images where the background is significant (like dark text on a light background), min pooling might emphasize background features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "66aa4d5d-ff4e-47ac-b6bc-a05e92e8f889",
   "metadata": {},
   "source": [
    "3.Discuss the concept of padding in CNN and its Significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e787c2-7781-4871-9495-7b01ebcde511",
   "metadata": {},
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is a technique where extra elements, typically zeros, are added to the borders of the input feature map before performing a convolution operation. It serves several important purposes:\n",
    "\n",
    "Preserving Spatial Information:\n",
    "\n",
    "A core function of CNNs is to extract features from spatial data like images. Convolution with filters of a specific size can lead to a loss of information at the edges of the input, especially with smaller filters or multiple convolution layers.\n",
    "Padding creates a buffer zone around the original data, allowing the filter to \"see\" some of the neighboring pixels even at the borders. This ensures that features present near the edges are not discarded and contribute to the convolution process.\n",
    "Controlling Output Size:\n",
    "\n",
    "Without padding, the size of the output feature map after a convolution operation typically shrinks compared to the input. This can be undesirable in certain network architectures.\n",
    "By strategically choosing the amount of padding, you can control the output size. Specific padding techniques, like \"same padding,\" can ensure that the output has the same dimensions as the input, making it easier to design CNNs with a predefined number of layers.\n",
    "Efficient Feature Extraction:\n",
    "\n",
    "Padding allows the filters to interact with all elements of the input data, potentially leading to more efficient feature extraction. Every pixel in the input has an equal opportunity to contribute to the formation of the output feature map.\n",
    "This can be particularly beneficial for capturing intricate features that might reside close to the edges of the image.\n",
    "Training Stability:\n",
    "\n",
    "Padding can also contribute to the stability of the training process in CNNs. By maintaining a consistent feature map size throughout the network, you can avoid drastic changes in the distribution of activations during backpropagation, which can improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7da8e0-7580-460b-862c-e7ffd3d4dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d47ec51e-1395-4b43-962c-8a8dd348cb76",
   "metadata": {},
   "source": [
    "4.Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33757f-c058-43bf-980d-507111c67126",
   "metadata": {},
   "source": [
    "Effect on Output Size:\n",
    "\n",
    "Zero-Padding: This type of padding adds zeros around the borders of the input feature map. The amount of padding added is determined by a specific formula that considers the filter size, stride, and desired output size. There are two main approaches:\n",
    "Same Padding: Aims to maintain the same output size as the input size. This is achieved by calculating the padding amount to compensate for the shrinking effect of the convolution operation.\n",
    "Custom Padding: Allows for more control over the output size. You can specify the number of zeros to add, leading to an output size that might be larger or smaller than the input depending on the padding value.\n",
    "Valid Padding: This approach, also known as \"no padding,\" skips adding any extra elements to the input. The convolution is performed only on the valid portion of the data, which is the area covered entirely by the filter without exceeding the image boundaries. Consequently, the output feature map will have a reduced size compared to the input.\n",
    "Impact on Feature Extraction:\n",
    "\n",
    "Zero-Padding: By including neighboring pixels through padding, zero-padding allows the filter to capture features present near the edges of the input. This can be beneficial for tasks where edge information is crucial.\n",
    "Valid Padding: Since valid padding only uses the central valid region, features close to the edges are not considered during convolution. This can lead to a loss of information, especially for smaller filters or with multiple convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939b26e-0837-40f2-afd1-5994efcb53a9",
   "metadata": {},
   "source": [
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9ce9e3f-6199-47ce-aa0e-ec032d6bd571",
   "metadata": {},
   "source": [
    "1. provide a brief overview of LetNet-5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94fa5c1-737a-46db-a10b-34fb1b4d8778",
   "metadata": {},
   "source": [
    "LetNet-5, developed in 1998 by Yann LeCun and colleagues, is a pioneering convolutional neural network (CNN) architecture known for its simplicity and effectiveness in image recognition tasks, particularly handwritten digit classification. Here's a quick overview:\n",
    "\n",
    "Structure:\n",
    "\n",
    "LeNet-5 consists of seven layers:\n",
    "Three convolutional layers for feature extraction\n",
    "Two pooling layers for dimensionality reduction\n",
    "Two fully-connected layers for classification\n",
    "Functionality:\n",
    "\n",
    "The input is a grayscale image (typically 32x32 pixels).\n",
    "Convolutional layers with learnable filters scan the image, extracting low-level features like edges and lines.\n",
    "Pooling layers downsample the feature maps, reducing computational cost and promoting translation invariance (recognizing objects regardless of small position shifts).\n",
    "Fully-connected layers combine the extracted features from previous layers and perform the final classification into digits (0-9).\n",
    "Significance:\n",
    "\n",
    "LeNet-5 paved the way for modern CNN architectures.\n",
    "It demonstrated the power of CNNs for image recognition tasks.\n",
    "Its relatively simple design makes it a good starting point for understanding CNN fundamentals."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4ab734f-f27a-4874-a483-f191243872c0",
   "metadata": {},
   "source": [
    "2.Describe the key components of LetNet-5 and their respective purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1584e-c8af-4255-86b0-f2ba586d761d",
   "metadata": {},
   "source": [
    "LeNet-5, a foundational convolutional neural network (CNN), is known for its role in pioneering image recognition. Here's a breakdown of its key components and their purposes:\n",
    "\n",
    "1. Convolutional Layers (3 total):\n",
    "\n",
    "These layers are the heart of feature extraction in LeNet-5. They use filters (small learnable kernels) to slide across the input image, detecting specific patterns and generating feature maps.\n",
    "Each convolutional layer has multiple filters, allowing it to learn various features like edges, lines, and shapes.\n",
    "The number of filters increases in subsequent layers, enabling the network to capture progressively more complex features.\n",
    "2. Pooling Layers (2 total):\n",
    "\n",
    "These layers downsample the feature maps generated by the convolutional layers. This serves two purposes:\n",
    "Dimensionality Reduction: Reduces the number of elements in the data, making the network more efficient and less prone to overfitting.\n",
    "Spatial Invariance: Makes the network less sensitive to small shifts in the position of objects within the image. Pooling summarizes information from a local region, allowing the network to recognize a feature even if its location varies slightly.\n",
    "LeNet-5 typically uses average pooling, which takes the average value within a specific region of the feature map.\n",
    "3. Activation Functions:\n",
    "\n",
    "After each convolution and pooling operation, an activation function is applied element-wise to the output. These functions introduce non-linearity into the network, allowing it to learn more complex relationships between features.\n",
    "LeNet-5 commonly uses the tanh (hyperbolic tangent) activation function, which outputs values between -1 and 1.\n",
    "4. Fully-Connected Layers (2 total):\n",
    "\n",
    "Unlike convolutional layers that operate on local regions, fully-connected layers connect all neurons from the previous layer to every neuron in the current layer. This allows for global information processing and integration of features extracted earlier.\n",
    "The first fully-connected layer in LeNet-5 serves as a hidden layer, further processing the combined features.\n",
    "The final fully-connected layer has one neuron for each output class (typically 10 for classifying digits 0-9). It uses a softmax activation function to predict the probability of the input image belonging to each class."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7550bc96-d574-4377-a575-07d3963f7541",
   "metadata": {},
   "source": [
    "4.Implement LetNET-5 using a deep learning framework of your choice (eg,tensorflow,PyTorch,and train it on a publicaly avaliable dadset(eg,MNIST).Evaluate its performance and provide insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a0d4e09-0a74-4bd6-b9e1-bf1544d9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Additional imports for plotting (optional)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1035a4f-114f-41be-a707-34259e648eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (optional, but recommended)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape to add channel dimension (MNIST is grayscale)\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# One-hot encode labels (optional, but recommended for categorical crossentropy)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a43043b1-10d0-4345-b537-8232eb2ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  Conv2D(filters=6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "  Conv2D(filters=16, kernel_size=(5, 5), strides=(1, 1), activation='tanh'),\n",
    "  MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(units=120, activation='tanh'),\n",
    "  Dense(units=84, activation='tanh'),\n",
    "  Dense(units=10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1986a3d4-134d-4df4-ac1a-2a919240ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5087674d-a259-42d6-a98f-18e7ad88744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.8949 - loss: 0.3607 - val_accuracy: 0.9812 - val_loss: 0.0604\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9813 - loss: 0.0624 - val_accuracy: 0.9839 - val_loss: 0.0502\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0422 - val_accuracy: 0.9844 - val_loss: 0.0515\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9901 - loss: 0.0312 - val_accuracy: 0.9872 - val_loss: 0.0383\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9915 - loss: 0.0245 - val_accuracy: 0.9865 - val_loss: 0.0419\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9939 - loss: 0.0185 - val_accuracy: 0.9872 - val_loss: 0.0432\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9949 - loss: 0.0167 - val_accuracy: 0.9859 - val_loss: 0.0451\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9955 - loss: 0.0140 - val_accuracy: 0.9861 - val_loss: 0.0438\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9954 - loss: 0.0141 - val_accuracy: 0.9880 - val_loss: 0.0429\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0092 - val_accuracy: 0.9879 - val_loss: 0.0455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2c534bca00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ea9c5-05f5-448d-a6a5-96d3b7701859",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edac5ef-4f58-491b-9604-6d8bd8b92e52",
   "metadata": {},
   "source": [
    "## TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aa3fe9a-7d2c-4825-809c-9022c98bb4ae",
   "metadata": {},
   "source": [
    "1.present an overiew of the ALexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b830d-e6af-4fef-96d5-b706da3b40da",
   "metadata": {},
   "source": [
    "AlexNet, a pioneering convolutional neural network (CNN) introduced in 2012, revolutionized the field of computer vision. Here's a breakdown of its key components:\n",
    "\n",
    "Convolutional Layers (Stacked for Feature Extraction):\n",
    "\n",
    "The core of AlexNet lies in its convolutional layers. These layers use filters (small learnable kernels) that slide across the input image, detecting specific patterns and generating feature maps.\n",
    "AlexNet employs multiple convolutional layers stacked one after another. Each layer has multiple filters, allowing it to learn various features like edges, lines, and shapes.\n",
    "The number of filters typically increases in subsequent layers, enabling the network to capture progressively more complex features from simple edges to object parts and eventually entire objects.\n",
    "Pooling Layers (Dimensionality Reduction and Invariance):\n",
    "\n",
    "Following convolutional layers, AlexNet incorporates pooling layers. These layers downsample the feature maps, reducing their dimensionality and computational cost. This makes the network more efficient and less prone to overfitting.\n",
    "Pooling also introduces some level of spatial invariance. This means the network becomes less sensitive to small shifts in the position of objects within the image. Even if an object is slightly off-center, the pooling operation ensures the relevant features are captured.\n",
    "AlexNet typically uses max pooling, which takes the maximum value within a specific region of the feature map.\n",
    "Activation Functions (Adding Non-linearity):\n",
    "\n",
    "After each convolution and pooling operation, an activation function is applied element-wise to the output. These functions introduce non-linearity into the network, allowing it to learn more complex relationships between features.\n",
    "AlexNet's innovative use of the ReLU (Rectified Linear Unit) activation function was a key contributor to its success. ReLU outputs the input value if it's positive, and zero otherwise. This simpler function allows for faster training compared to traditional functions like tanh, without sacrificing performance.\n",
    "Fully-Connected Layers (Classification):\n",
    "\n",
    "Unlike convolutional layers that operate on local regions, fully-connected layers connect all neurons from the previous layer to every neuron in the current layer. This allows for global information processing and integration of features extracted earlier.\n",
    "AlexNet uses fully-connected layers towards the end of the network. These layers receive the flattened output from the previous convolutional layers and process them to generate class probabilities.\n",
    "The final fully-connected layer has one neuron for each output class (e.g., 1000 for ImageNet). A softmax activation function is applied to this layer, producing a probability distribution indicating the likelihood of the input image belonging to each class."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a514cca7-cda7-4a12-a13c-19f627753ea0",
   "metadata": {},
   "source": [
    "2.Explanin the architecture  innovations introduced in AlexNet that contribute to its breakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773c3a9-cd57-433f-8333-5490845673a0",
   "metadata": {},
   "source": [
    "AlexNet's breakthrough performance in image recognition stemmed from several architectural innovations that addressed key challenges in CNNs at the time. Here's a deeper dive into these innovations:\n",
    "\n",
    "Rectified Linear Unit (ReLU) Activation Function:\n",
    "\n",
    "Traditional CNNs often used the tanh (hyperbolic tangent) activation function, which outputs values between -1 and 1. However, tanh can suffer from vanishing gradients during training, making it slow to learn.\n",
    "AlexNet introduced the ReLU activation function, which outputs the input value if it's positive, and zero otherwise. This simpler function allows for faster training convergence compared to tanh, while maintaining good performance.\n",
    "Overfitting Prevention Techniques:\n",
    "\n",
    "Overfitting is a major problem in deep learning where the model learns the training data too well and performs poorly on unseen data. AlexNet introduced two key techniques to combat this:\n",
    "Dropout: During training, a random subset of activations in a layer is set to zero. This forces the network to learn features from different sets of neurons, preventing overreliance on specific connections and improving generalization to unseen data.\n",
    "Data Augmentation: Artificially expanding the training data by applying random transformations like cropping, flipping, scaling, and color jittering increases the model's exposure to variations and helps it learn features that are robust to small changes in the image.\n",
    "Deep Network Architecture with Overlapping Pooling:\n",
    "\n",
    "AlexNet utilized a deeper network architecture compared to previous CNNs, stacking multiple convolutional layers with varying filter sizes and strides. This allows the network to learn features at different scales and levels of abstraction.\n",
    "Unlike prior approaches that used non-overlapping pooling, AlexNet employed overlapping pooling. This increases the effective receptive field of the network, enabling it to capture features from larger image regions and improve its ability to understand spatial relationships between pixels."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd03f575-f5e4-4882-9df5-80a5e0bb2db2",
   "metadata": {},
   "source": [
    "4.Implement AlexNet using a deep learnig framework of your choice and evaluavte its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba240449-807e-4ae7-82f2-04946212fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80434b8b-2803-4517-a56e-7756bb57bb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.0.0\n",
      "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.28.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.13.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.63.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f9c23b2-8af0-431f-9ba6-d417c0d2690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e9df9d-af6c-4375-a6ee-c5fa4e0f222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "num_classes = 10  # Replace with the actual number of classes in your dataset\n",
    "\n",
    "# 1st Convolutional Layer \n",
    "model.add(Conv2D(filters = 96, input_shape = (224, 224, 3),  \n",
    "            kernel_size = (11, 11), strides = (4, 4),  \n",
    "            padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling  \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), \n",
    "            strides = (2, 2), padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 2nd Convolutional Layer \n",
    "model.add(Conv2D(filters = 256, kernel_size = (11, 11),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n",
    "            padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 3rd Convolutional Layer \n",
    "model.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 4th Convolutional Layer \n",
    "model.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 5th Convolutional Layer \n",
    "model.add(Conv2D(filters = 256, kernel_size = (3, 3),  \n",
    "            strides = (1, 1), padding = 'valid')) \n",
    "model.add(Activation('relu')) \n",
    "# Max-Pooling \n",
    "model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n",
    "            padding = 'valid')) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# Flattening \n",
    "model.add(Flatten()) \n",
    "  \n",
    "# 1st Dense Layer \n",
    "model.add(Dense(4096, input_shape = (224*224*3, ))) \n",
    "model.add(Activation('relu')) \n",
    "# Add Dropout to prevent overfitting \n",
    "model.add(Dropout(0.4)) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# 2nd Dense Layer \n",
    "model.add(Dense(4096)) \n",
    "model.add(Activation('relu')) \n",
    "# Add Dropout \n",
    "model.add(Dropout(0.4)) \n",
    "# Batch Normalisation \n",
    "model.add(BatchNormalization()) \n",
    "  \n",
    "# Output Softmax Layer \n",
    "model.add(Dense(num_classes)) \n",
    "model.add(Activation('softmax')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e15bce-ed1b-4949-96c0-a5b3f9735c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243cfcd-1257-4c23-8b43-bfcbff306a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24331870-cfa6-46eb-9a7f-411322abb26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9ca1c-3b0a-416f-ab01-a39aceaccdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0f362-d9a9-4b22-9f89-84f4ecde61ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ece7f7-5c63-48c3-beb5-8f38ba4e3a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cdd18-5ce8-4d64-953d-e094cca4b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab572221-1580-402a-ac7a-24c701d01b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab03c6c-035d-4d90-a2cb-5284110a788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806049e-c7cd-4c3e-ba42-9cb4a9f6335b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45588871-45cb-47c7-8163-01af72f5c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbba8b5-b4e1-4b56-9e0a-1066497da729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
