{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678e110f-e7f9-4818-9c7b-cf34da8ed99b",
   "metadata": {},
   "source": [
    "# Q1. Theory and Concepts:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3d95adb-c4b1-4de3-83e0-abccf97148a6",
   "metadata": {},
   "source": [
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159ef54-9fa0-4810-b312-223415349790",
   "metadata": {},
   "source": [
    "Normalization: During training, batch norm takes a mini-batch of data and calculates the mean and standard deviation of the activations for each feature (channel) within that batch. It then subtracts the mean and divides by the standard deviation, effectively centering and re-scaling the activations to a standard normal distribution.\n",
    "\n",
    "Mini-batch: It's important to note that normalization is done using the statistics from the mini-batch, not the entire dataset. This is because using the entire dataset for normalization at every step would be computationally expensive and impractical with stochastic gradient descent (SGD), a common optimization algorithm for ANNs.\n",
    "\n",
    "Learnable Parameters:  While batch normalization standardizes the activations, it can also introduce a shift in the distribution. To address this, two learnable parameters are introduced for each feature: gamma and beta. These parameters allow the network to learn how much to scale and shift the normalized activations back to the original scale, if necessary.\n",
    "\n",
    "Benefits of Batch Normalization:\n",
    "\n",
    "Faster Training: By stabilizing the distribution of activations across layers, batch normalization allows for using higher learning rates during training. This can significantly speed up the convergence process.\n",
    "\n",
    "Reduced Internal Covariate Shift: In deep neural networks, the distribution of activations can change drastically between layers during training. This phenomenon, known as internal covariate shift, can make it difficult for the network to learn effectively. Batch normalization helps mitigate this issue by normalizing the activations at each layer.\n",
    "\n",
    "Improved Regularization: Batch normalization can act as a form of regularization, reducing the network's susceptibility to overfitting. This is because the network is forced to learn more independent features due to the normalized activations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a994859-3f57-4256-8593-a3b1965c9cbd",
   "metadata": {},
   "source": [
    "2. Describe the benefits of using batch normalization during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2d541-c849-41f1-b127-89a35775f5fc",
   "metadata": {},
   "source": [
    "Batch normalization offers several benefits during training artificial neural networks:\n",
    "\n",
    "Faster Training: Batch normalization stabilizes the learning process by normalizing the activations across layers. This allows you to use higher learning rates, which can significantly speed up convergence. Traditional gradient descent with large learning rates can cause gradients to explode or vanish, hindering the learning process. Batch normalization prevents this by keeping the activations in a consistent range.\n",
    "\n",
    "Reduced Internal Covariate Shift: As a network trains, the distribution of activations flowing through it can change drastically between layers. This is known as internal covariate shift. Batch normalization helps mitigate this issue by normalizing activations at each layer. By forcing the activations to a standard normal distribution, it essentially resets the target for each layer after the previous layer's updates, making the learning process more efficient.\n",
    "\n",
    "Improved Regularization: Batch normalization can act as a form of regularization, reducing the network's tendency to overfit. Overfitting occurs when a model memorizes the training data too well and performs poorly on unseen data. By normalizing activations, batch normalization introduces a slight element of randomness, making the network less sensitive to specific weight values and encouraging it to learn more robust features. In some cases, this can even reduce the need for dropout, another common regularization technique.\n",
    "\n",
    "Reduced Need for Careful Initialization:  Initializing weights in a neural network with good starting values is crucial for proper training. Batch normalization can alleviate some of this sensitivity. By normalizing activations, it makes the network less dependent on the specific initial weight values, allowing for a wider range of reasonable starting points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e74fb730-a59f-40b5-8786-0f2073f5748f",
   "metadata": {},
   "source": [
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e802100-3602-4ea6-a627-d210af36027c",
   "metadata": {},
   "source": [
    "1. Normalization Step:\n",
    "    During training, a mini-batch of data is fed into the network.\n",
    "For each feature (channel) within the activations of a particular layer, the mean (µ) and standard deviation (σ) are calculated across the entire mini-batch.\n",
    "Each activation value (x) is then normalized by subtracting the mean (µ) and dividing by the standard deviation (σ):\n",
    "    normalized_x = (x - µ) / √σ\n",
    "    2. Learnable Parameters:While normalization ensures a consistent distribution across mini-batches, it can also introduce a shift in the distribution of activations. To address this, batch normalization introduces two learnable parameters for each feature: gamma (γ) and beta (β).\n",
    "\n",
    "Gamma (γ): This parameter scales the normalized activations. After normalization, the activations might have a lower standard deviation than desired for optimal learning. Gamma allows the network to learn how much to scale the activations back up, essentially controlling the variance.\n",
    "\n",
    "Beta (β): This parameter shifts the normalized activations. Normalization centers the activations around zero, but for optimal learning, they might need to be shifted to a different mean value. Beta allows the network to learn this shift and bring the activations back to a more suitable range.\n",
    "\n",
    "The final output of a batch normalization layer is calculated using these learnable parameters:\n",
    "\n",
    "output = γ * normalized_x + β\n",
    "This allows the network to adapt the normalized activations and retain the information they hold while maintaining a more stable learning process.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Normalization with mean and standard deviation is done using the mini-batch statistics, not the entire dataset, for efficiency.\n",
    "Gamma and beta are updated during backpropagation along with other weights in the network.\n",
    "At inference time (using the trained model on new data), batch normalization uses estimates of the mean and standard deviation from the training process (often exponential moving averages) for normalization, as mini-batch statistics wouldn't be representative of the entire unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29076b19-c727-45e3-8eff-006020e2f131",
   "metadata": {},
   "source": [
    "# Q2.Implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed57f0f-04d6-4d37-99ae-332c62762ac2",
   "metadata": {},
   "source": [
    "1. Dataset and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863819b8-0171-4b6a-9c79-e6b0fc88f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.28.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9953baf0-2a82-4027-9169-07885b066787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape images to a format suitable for the neural network (e.g., flatten 28x28 images to 784-dimensional vectors)\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "# Normalize pixel values to the range [0, 1] for better training\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels for categorical classification\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4945ba-3f85-40bc-8d3f-9c70c8ed4faf",
   "metadata": {},
   "source": [
    "2. Simple Feedforward Neural Network without Batch Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7097444a-911c-47c7-b3cd-dd91ee63df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9026 - loss: 0.3403 - val_accuracy: 0.9667 - val_loss: 0.1081\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9738 - loss: 0.0837 - val_accuracy: 0.9723 - val_loss: 0.0900\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0492 - val_accuracy: 0.9791 - val_loss: 0.0702\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9881 - loss: 0.0378 - val_accuracy: 0.9793 - val_loss: 0.0728\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0237 - val_accuracy: 0.9795 - val_loss: 0.0682\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0177 - val_accuracy: 0.9794 - val_loss: 0.0690\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9945 - loss: 0.0163 - val_accuracy: 0.9803 - val_loss: 0.0759\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0123 - val_accuracy: 0.9803 - val_loss: 0.0806\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 0.9798 - val_loss: 0.0877\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0105 - val_accuracy: 0.9800 - val_loss: 0.0878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f49d0571360>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(512, activation='relu', input_shape=(784,)),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330af387-5ad2-4549-8319-cf804e9ff202",
   "metadata": {},
   "source": [
    "4. Implementing Batch Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f948a1e-b369-477d-ba00-485ed7e7845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9080 - loss: 0.3035 - val_accuracy: 0.9686 - val_loss: 0.0996\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0994 - val_accuracy: 0.9707 - val_loss: 0.1003\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0746 - val_accuracy: 0.9775 - val_loss: 0.0731\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9810 - loss: 0.0590 - val_accuracy: 0.9786 - val_loss: 0.0708\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9846 - loss: 0.0483 - val_accuracy: 0.9761 - val_loss: 0.0791\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0383 - val_accuracy: 0.9783 - val_loss: 0.0730\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9883 - loss: 0.0346 - val_accuracy: 0.9792 - val_loss: 0.0692\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0309 - val_accuracy: 0.9791 - val_loss: 0.0759\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0284 - val_accuracy: 0.9783 - val_loss: 0.0738\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9904 - loss: 0.0272 - val_accuracy: 0.9796 - val_loss: 0.0732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4a6b6adf60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model_bn = Sequential([\n",
    "  Dense(512, activation='relu'),\n",
    "  BatchNormalization(),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_bn.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987408a-8615-43b5-842e-1992dc165ad5",
   "metadata": {},
   "source": [
    "5. Training and Validation Performance Comparison:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6110f6d3-e89e-45e7-8ed7-a53936f4ae2f",
   "metadata": {},
   "source": [
    "Train the model with batch normalization for 10 epochs and compare the training and validation accuracy/loss curves with the model without batch normalization. You should observe:\n",
    "\n",
    "Faster Convergence: The model with batch normalization might achieve higher validation accuracy earlier compared to the model without it.\n",
    "Improved Validation Accuracy: The model with batch normalization might reach a higher final validation accuracy than the one without it.\n",
    "Reduced Training Loss: The model with batch normalization might exhibit a smoother loss curve with less fluctuation during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13fe6a-70e0-4f28-99b5-99322bf4d66d",
   "metadata": {},
   "source": [
    "6. Impact of Batch Normalization:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a2cb473-9bec-4b2c-b24d-ab00a35f3cdc",
   "metadata": {},
   "source": [
    "Faster Training: Batch normalization stabilizes the learning process, allowing for higher learning rates which can lead to faster convergence.\n",
    "Improved Generalization: By mitigating internal covariate shift, batch normalization can help the network learn more robust features and improve generalization performance on unseen data (higher validation accuracy).\n",
    "Reduced Overfitting: Batch normalization acts as a regularizer, reducing the model's susceptibility to overfitting, potentially leading to a smoother loss curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a0fa0-36e1-4d08-b1a5-9756d5324496",
   "metadata": {},
   "source": [
    "# Q3. Experimentation and Analysis:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5e22c79-4f62-4af9-b352-ec27e80abca1",
   "metadata": {},
   "source": [
    "1.Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb6404cd-3d8c-4c66-aa7b-63551a38c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical  # Import for one-hot encoding\n",
    "\n",
    "try:\n",
    "  # Attempt to load TensorFlow and Keras\n",
    "  from tensorflow.keras import backend as K  # Optional import for some setups\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "  print(\"Error: TensorFlow or Keras not found. Please install them using 'pip install tensorflow'.\")\n",
    "  quit()\n",
    "\n",
    "# Load MNIST dataset\n",
    "try:\n",
    "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "except OSError as e:\n",
    "  print(f\"Error: MNIST dataset not found. Please download it manually or ensure the data path is correct.\")\n",
    "  quit()\n",
    "\n",
    "# Preprocess data (normalize pixel values to [0, 1])\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape data for feedforward network\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)  # Encode test labels as well\n",
    "\n",
    "# Define the model architecture (simple feedforward network)\n",
    "def create_model():\n",
    "  model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(28, 28, 1)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Only 10 units for class probabilities\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "# Define a function to train the model with a given batch size\n",
    "def train_model(batch_size):\n",
    "  model = create_model()\n",
    "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  # Track training and validation history for analysis\n",
    "  history = model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_data=(x_test, y_test), verbose=2)\n",
    "  return history\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128]  # Adjust this list as neededtraining_\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c761d788-4e73-433b-b508-9f1bc94545b8",
   "metadata": {},
   "source": [
    "2.Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73030c-be9f-48d3-8c99-3f0eaf24efb6",
   "metadata": {},
   "source": [
    "Faster Convergence:\n",
    "    By normalizing the activations of each layer, BN reduces internal covariate shift, a phenomenon where the distribution of activations changes throughout the network during training.     This makes the gradient updates more stable and allows the network to learn faster by alleviating the vanishing or exploding gradient problem.\n",
    "Higher Learning Rates: \n",
    "    BN enables the use of higher learning rates, which can significantly accelerate training. Without BN, using high learning rates can lead to unstable gradients and divergence.\n",
    "Reduced Need for Other Regularization Techniques:\n",
    "    BN inherently acts as a regularizer by introducing slight noise during training through batch statistics. This can help reduce overfitting, potentially leading to better                 generalization on unseen data. Some regularization techniques (like dropout) might be less necessary when using BN.\n",
    "Improved Initialization Sensitivity:\n",
    "    Neural networks can be sensitive to the initial weight values. BN can help alleviate this sensitivity by normalizing the activations, making the network less dependent on specific initializations.\n",
    "    \n",
    "Potential Limitations of Batch Normalization\n",
    "While BN offers significant benefits, there are also some potential limitations to consider:\n",
    "\n",
    "Increased Computational Cost: \n",
    "    BN introduces additional computations during training to calculate batch statistics and normalize activations. This can lead to slower training, especially on large datasets or           limited hardware.\n",
    "Dependence on Batch Size: \n",
    "    BN relies on batch statistics, which can be less accurate for smaller batch sizes. This can potentially affect the effectiveness of BN and might necessitate adjusting hyperparameters     like the learning rate based on the chosen batch size.\n",
    "Not a Silver Bullet:\n",
    "    BN is not a guaranteed solution for all training problems. While it can significantly improve training in many cases, it might not always lead to better performance. Other factors       like model architecture and hyperparameter tuning still play a crucial role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545962c9-5d91-43f7-b32f-0c2203bc8ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
