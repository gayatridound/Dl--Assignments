{
 "cells": [
  {
   "cell_type": "raw",
   "id": "35e0c331-6290-4a42-9ac6-a6371071174f",
   "metadata": {},
   "source": [
    "1.What are the objectives of using selective search in R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97625e88-ede3-4d06-b6bb-0c4ae49763f7",
   "metadata": {},
   "source": [
    "Selective search is a key component used in R-CNN for object detection. Here's how it helps:\n",
    "\n",
    "Reduced Search Space: Compared to a sliding window approach that scans the entire image with many overlapping windows, selective search generates a smaller set of candidate regions (bounding boxes) that are more likely to contain objects. This significantly reduces the computational cost required for subsequent processing steps in R-CNN.\n",
    "\n",
    "High Recall: Selective search prioritizes finding most, if not all, objects in the image. This is achieved by strategically merging similar image segments, leading to a high probability of capturing objects of various sizes and orientations.\n",
    "\n",
    "Trade-off: While selective search offers high recall, it can also generate a significant number of proposals that don't actually contain objects (low precision).  However, these false positives are handled later in the R-CNN pipeline by a classifier that discards them."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5f37c7a-e7fd-4513-b88d-34e6147495ab",
   "metadata": {},
   "source": [
    "2.Explain the following phases invovled in R-CNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d8c0c-d497-4f42-85d8-800029b6e8f8",
   "metadata": {},
   "source": [
    "a.Region proposal:\n",
    "    \n",
    "  \n",
    "The RPN operates on the shared convolutional feature map extracted from the input image (see point 2).\n",
    "It efficiently generates candidate object bounding boxes (regions where objects might be present) using small convolutional filters that slide across the feature map.\n",
    "The RPN also predicts a foreground object score (objectness score) for each proposal, indicating the likelihood that the region contains an object.\n",
    "\n",
    "b.Warping and Resizing:\n",
    "   \n",
    "\n",
    "Images might be wrapped (padded) with pixels to ensure a consistent input size for the CNN. This is especially common when training the model on a dataset with images of varying resolutions.\n",
    "Images might be resized to a specific resolution suitable for the CNN architecture. Resizing can help reduce computational cost and ensure consistent feature maps.\n",
    "    Post-processing (potential):    \n",
    "After detection, bounding boxes might be adjusted to fit the original image resolution if resizing was done in pre-processing. This essentially unwraps the detections.\n",
    "\n",
    "\n",
    "c.Pre trained CNN architecture:\n",
    "    These are deep convolutional neural networks (CNNs) that have already been trained on massive image datasets like ImageNet, which contains millions of labeled images.\n",
    "During training, these CNNs learn powerful feature representations that capture essential characteristics of objects and scenes within images.\n",
    "\n",
    "\n",
    "d.pre trained SVM models:\n",
    "    Support Vector Machines (SVMs) are powerful algorithms for classification tasks. However, they are not inherently designed for feature extraction like CNNs.\n",
    "While SVMs can be used for object detection in specific scenarios, they typically require handcrafted features to be engineered from the image data. This feature engineering process can be time-consuming and domain-specific.\n",
    "\n",
    "\n",
    "e. Clean UP:\n",
    "    After detection, bounding boxes might be adjusted back to the original image resolution if resizing was done pre-processing (essentially unwrapping the detections).\n",
    "    \n",
    "    \n",
    "f.Implementataion of bounding box:\n",
    "    Bounding boxes are typically represented by coordinates that define a rectangle around the detected object in the image.\n",
    "There are two common ways to represent these coordinates:\n",
    "(x1, y1, x2, y2): This specifies the top-left corner (x1, y1) and bottom-right corner (x2, y2) of the rectangle.\n",
    "(center_x, center_y, width, height): This defines the center point (center_x, center_y) of the box and its width and height.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "267197cd-5732-4f37-ae72-33ef0fec6ed4",
   "metadata": {},
   "source": [
    " 3. what are  the possible pre trained CNNs we can use in pre trained CNN archicture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de83faf-b303-4063-967f-e72983245eac",
   "metadata": {},
   "source": [
    "VGG (Visual Geometry Group):\n",
    "\n",
    "Known for its stacked convolutional layers, VGG models (e.g., VGG16) were popular choices in earlier R-CNN implementations.\n",
    "Advantages: Relatively simple architecture, good performance on some datasets.\n",
    "Disadvantages: Can be computationally expensive due to the depth of the network.\n",
    "\n",
    "ResNet (Residual Network):\n",
    "\n",
    "Introduces skip connections that help alleviate the vanishing gradient problem in deep networks.\n",
    "ResNet models (e.g., ResNet-50) offer good performance and are widely used in R-CNN variants like Faster R-CNN.\n",
    "Advantages: Addresses vanishing gradient problem, often achieves better accuracy than VGG for similar computational cost.\n",
    "Disadvantages: Can still be computationally expensive for resource-constrained environments.\n",
    "\n",
    "Inception (Google Inception Network):\n",
    "\n",
    "Employs inception modules with efficient convolutional configurations.\n",
    "Inception models (e.g., Inception-v3) can be used for R-CNN, especially when computational efficiency is a concern.\n",
    "Advantages: More efficient architecture compared to VGG for similar accuracy, potentially suitable for resource-limited settings.\n",
    "Disadvantages: Might not always achieve the highest accuracy compared to other options."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd4df473-8bc0-4a0c-8a7d-93af7e2dcead",
   "metadata": {},
   "source": [
    "4.How is SVM implemented in the R-CNN framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73bb09-8a22-4bc3-8cb2-4eaae900fe23",
   "metadata": {},
   "source": [
    "SVM Limitations: While SVMs are powerful classification algorithms, they are not inherently designed for feature extraction like CNNs. They typically require handcrafted features to be engineered from the image data, which can be:\n",
    "\n",
    "Time-consuming: The feature engineering process can be labor-intensive and require domain-specific knowledge.\n",
    "Less Efficient: Engineered features might not capture the same level of detail and complexity as features learned by CNNs.\n",
    "\n",
    "pen_spark\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b142295f-64ef-4ff0-bd0f-e067bee2d794",
   "metadata": {},
   "source": [
    "5.How does Non-maximum Suppression work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e17670-95b1-4f8d-9157-5b890d9232ec",
   "metadata": {},
   "source": [
    "Sorting Bounding Boxes: The algorithm typically starts by sorting all the proposed bounding boxes based on a specific criterion:\n",
    "\n",
    "Confidence Score: Often, the primary sorting factor is the confidence score assigned to each bounding box by the object detection model. The higher the score, the more likely the box contains the correct object.\n",
    "Intersection-over-Union (IoU): In some cases, NMS might use IoU (a measure of overlap between bounding boxes) as the sorting factor.\n",
    "Iterative Processing: NMS goes through the sorted bounding boxes one by one:\n",
    "\n",
    "Consider the Top Box: It starts with the bounding box with the highest confidence score (or highest IoU, depending on the sorting criteria). This box is considered the most likely detection for a specific object.\n",
    "\n",
    "Evaluate Overlap: NMS calculates the IoU (overlap) between the top box and all remaining bounding boxes. IoU is calculated as the area of intersection between two boxes divided by the area of their union.\n",
    "\n",
    "Suppression Threshold: If the IoU between the top box and any remaining box exceeds a predefined threshold, the overlapping box is considered a redundant detection and suppressed (removed from the final list of detections).\n",
    "\n",
    "Move to Next Box: NMS then proceeds to the next highest-scoring (or highest IoU) bounding box in the sorted list and repeats the process of evaluating overlap and potentially suppressing boxes.\n",
    "\n",
    "Final Output: After iterating through all the boxes, NMS provides a refined set of non-overlapping bounding boxes with the highest confidence scores (or highest IoU) for each detected object."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7e5f487-d136-4778-a0bf-49de5c83939a",
   "metadata": {},
   "source": [
    "6.How Fast R-CNN is better than R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db8930-0b47-4df2-a011-189551853523",
   "metadata": {},
   "source": [
    "R-CNN Bottleneck: Selective Search\n",
    "\n",
    "R-CNN relies on a technique called selective search to generate candidate regions (bounding boxes) that might contain objects.\n",
    "While selective search achieves high recall (finding most objects), it can be computationally expensive and generate a large number of proposals, many of which might not actually contain objects (low precision).\n",
    "Fast R-CNN's Advantage: Region Proposal Network (RPN)\n",
    "\n",
    "Faster R-CNN introduces the Region Proposal Network (RPN) as a key innovation.\n",
    "The RPN operates on the same shared convolutional feature map extracted from the input image (as used by the Fast R-CNN detector later).\n",
    "RPN efficiently generates candidate bounding boxes directly from the feature map using small convolutional filters. It also predicts an objectness score for each proposal, indicating the likelihood of an object being present."
   ]
  },
  {
   "cell_type": "raw",
   "id": "da5a94e1-3ddf-4929-a19b-d61894390591",
   "metadata": {},
   "source": [
    "7.Using mathamatical intuition ,explain ROI poolingg in fast R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac0442-43be-4fcb-9a36-7feb7b7ece65",
   "metadata": {},
   "source": [
    "Problem:\n",
    "\n",
    "Fast R-CNN uses a pre-trained CNN to extract a feature map from the input image. This feature map captures spatial information about the image at different scales.\n",
    "However, candidate object proposals generated by the Region Proposal Network (RPN) can have varying sizes and aspect ratios.\n",
    "We need a way to extract a fixed-size feature vector from the relevant region of the feature map for each proposal, regardless of its size. This feature vector will be used for classification and bounding box refinement.\n",
    "Solution: ROI Pooling:\n",
    "\n",
    "ROI pooling addresses this challenge by dividing the proposal region in the feature map into a grid of a predefined size (e.g., 7x7). Here's the intuition behind the process:\n",
    "\n",
    "Grid Division: Imagine overlaying a grid of equal-sized squares on the proposal region in the feature map. The size of each square depends on the chosen grid size (e.g., 7x7 for a 7x7 grid).\n",
    "\n",
    "Pooling Operation: For each cell in the grid, ROI pooling performs a specific pooling operation (like max pooling) on the corresponding area of the feature map within the proposal region.\n",
    "\n",
    "Max Pooling Intuition: Max pooling in this context helps capture the most dominant feature within each cell of the grid. For example, in a cell containing an object edge, max pooling will likely select the pixel with the strongest edge response.\n",
    "Fixed-Size Feature Vector: The output of ROI pooling is a fixed-size feature vector. Each element in the vector corresponds to the pooled value from a specific cell in the grid.\n",
    "Mathematical Representation:\n",
    "\n",
    "Let:\n",
    "\n",
    "F be the feature map extracted by the CNN.\n",
    "B be a bounding box proposal from the RPN.\n",
    "Gi be the grid cell 'i' within the proposal region of the feature map.\n",
    "S be the spatial size of the grid (e.g., 7 for a 7x7 grid)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c021bfac-46bf-483b-9a6c-073df9db5873",
   "metadata": {},
   "source": [
    "8. Explain the following processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b06c4e-bfde-4d2b-8896-f0c029446429",
   "metadata": {},
   "source": [
    "a.ROI projection\n",
    "\n",
    "n the context of RCNNs, the term used for projecting bounding boxes onto feature maps is ROI alignment or spatial transformation.\n",
    "Similar to ROI projection we discussed earlier (intended for financial contexts), ROI alignment aims to transform the proposed bounding boxes (ROIs) to align them with the feature maps generated by the CNN.\n",
    "ROI alignment improves the accuracy of RCNN models by ensuring better correspondence between proposed regions and the features used for object classification and bounding box refinement.\n",
    "It addresses the limitations of ROI pooling, which can lead to information loss due to misalignment.\n",
    "\n",
    "b.ROI pooling\n",
    "\n",
    "ROI pooling refers to a specific operation used to extract features from Regions of Interest (ROIs).\n",
    "In RCNN models like Fast R-CNN, candidate bounding boxes (ROIs) proposed by a separate network are fed into the system.\n",
    "These ROIs have varying sizes depending on the proposed object.\n",
    "ROI pooling addresses the challenge of feeding features extracted from these non-uniform ROIs into subsequent layers that require fixed-size inputs.\n",
    "ROI pooling enables processing features from ROIs of varying sizes within a unified framework.\n",
    "This allows subsequent layers in the RCNN architecture, like fully connected layers, to efficiently analyze these features for object classification and bounding box refinement."
   ]
  },
  {
   "cell_type": "raw",
   "id": "55cc02dc-13c2-4298-9e33-59d96c39ffea",
   "metadata": {},
   "source": [
    "9.In comparison with R-CNN ,why did the object classifier activation function change in fast R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd879b-ef1f-4c6f-8635-83c989fd95b8",
   "metadata": {},
   "source": [
    "R-CNN and Softmax:\n",
    "\n",
    "R-CNN relied on a softmax activation function in its classification layer.\n",
    "Softmax is handy for multi-class classification tasks, as it produces a probability distribution across all possible classes. In object detection, this might translate to classifying an image region as containing a cat, dog, or background (no object).\n",
    "Fast R-CNN and the Linear Activation Shift:\n",
    "\n",
    "To tackle this inefficiency, Fast R-CNN opted for a linear activation function in its classifier, replacing softmax.\n",
    "This change allows Fast R-CNN to perform proposal generation and classification within a single network pass. The linear layer generates scores for each class.\n",
    "A separate softmax layer then takes these scores and transforms them into final class probabilities.\n",
    "The Benefit of Linear Activation:\n",
    "\n",
    "The linear activation function enables Fast R-CNN to directly regress class scores, making it computationally faster compared to using softmax within the main network."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1caebf38-24dc-4196-9ba5-3a1c5b884003",
   "metadata": {},
   "source": [
    "10.What major changes in Faster R-CNN compared to Fast R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab76a6-8c54-4c5f-85a0-2a7a6bbaeb4f",
   "metadata": {},
   "source": [
    "Fast R-CNN - Reliant on External Proposal Generation:\n",
    "\n",
    "Fast R-CNN depends on an external proposal generation method like selective search to identify potential object regions (bounding boxes) in the image.\n",
    "This separate step adds complexity and can be a bottleneck for speed.\n",
    "Faster R-CNN - Introducing the Region Proposal Network (RPN):\n",
    "\n",
    "Faster R-CNN integrates a new module called the Region Proposal Network (RPN) within the same network architecture.\n",
    "The RPN operates on the feature maps extracted by the convolutional layers of the CNN.\n",
    "It predicts two outputs for each location in the feature map:\n",
    "The probability of that location being the center of an object.\n",
    "Adjustments (offsets) to refine potential bounding boxes for the object."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ab4da45-0bd0-41c0-b46e-8d27f9317ad8",
   "metadata": {},
   "source": [
    "12.Implement Faster R-CNN using 2017 COCO dataset .Train dataset, Val dataset and test dataset. you can use a pre-trained backbone network like ResNet or VGG for feature extraction. For refeence implement the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09bfb81-82a1-4870-81ff-46f73a667dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: torch==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (2.8.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->torchvision) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.1->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba19cd-dcde-4b72-b10a-cc29c035f392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eec94e-1de6-4e6c-9117-9f714c1c28c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff19428-e6e8-4752-8ce0-3f9190467aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968efbd2-b61e-4906-b745-d1feeaaa7d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "00a90187-bb10-4714-b63b-332d46c596be",
   "metadata": {},
   "source": [
    "a. Dataset Preparatin\n",
    "i. Dwnlad and preprcess the C^C^ dataset, including the anntatins and images.\n",
    "ii. Split the dataset int training and validatin sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac413c6-e698-4ca2-a14b-013bb49cc934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2424edf0-eb2a-4cd7-b533-35673850a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define Faster R-CNN architecture with pre-trained ResNet base\n",
    "class FasterRCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        # Load pre-trained ResNet with frozen weights\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4230dcc2-b631-49f2-83ad-23432192bb56",
   "metadata": {},
   "source": [
    "b. Model Architecture:\n",
    "i.Build a faster R-CNN model archicture using a pre-trained backbone(eg ResNet-50)for feature extraction.\n",
    "ii. Customise the RPN(Region proposal Network)and RCNN(Region- based Convolution Neural Network)heads as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfdd1f55-6f6e-442e-863e-d5d31970ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class FasterRCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet-50 backbone (freeze weights)\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Region Proposal Network (RPN)\n",
    "        self.rpn = RPN(in_features=self.backbone.fc.in_features, num_anchors=9)  # Assuming 9 anchor boxes\n",
    "\n",
    "        # RoI Pooling layer\n",
    "        self.roi_pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "\n",
    "        # Classification and Bounding Box Regression heads\n",
    "        self.cls_head = ClassificationHead(in_features=self.backbone.fc.in_features, num_classes=num_classes)\n",
    "        self.reg_head = RegressionHead(in_features=self.backbone.fc.in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass image through pre-trained backbone\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Pass features to RPN\n",
    "        rpn_proposals, rpn_scores = self.rpn(features)  # Get proposals and scores\n",
    "\n",
    "        # Perform RoI Pooling on features based on proposals\n",
    "        pooled_features = self.roi_pool(features, rpn_proposals.unsqueeze(1))\n",
    "\n",
    "        # Pass pooled features to classification and regression heads\n",
    "        cls_logits = self.cls_head(pooled_features)\n",
    "        bbox_reg = self.reg_head(pooled_features)\n",
    "\n",
    "        return rpn_proposals, rpn_scores, cls_logits, bbox_reg\n",
    "\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_features, num_anchors):\n",
    "        super(RPN, self).__init__()\n",
    "        # 3x3 convolutional layer for transforming features\n",
    "        self.conv = nn.Conv2d(in_features, 256, kernel_size=3, padding=1)\n",
    "        # Separate output layers for classification (objectness scores) and regression (bounding box offsets)\n",
    "        self.cls_logits = nn.Conv2d(256, num_anchors * 2, kernel_size=1)  # 2 for objectness (foreground/background)\n",
    "        self.reg_deltas = nn.Conv2d(256, num_anchors * 4, kernel_size=1)  # 4 for x, y, w, h offsets\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv(x))  # Apply ReLU activation\n",
    "        cls_logits = self.cls_logits(x)  # Objectness scores\n",
    "        reg_deltas = self.reg_deltas(x)  # Bounding box offsets\n",
    "        return cls_logits.reshape(cls_logits.size(0), -1, 2), reg_deltas.reshape(reg_deltas.size(0), -1, 4)\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        # Fully-connected layers for classification\n",
    "        self.fc1 = nn.Linear(in_features, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        cls_logits = self.fc2(x)  # Classification logits\n",
    "        return cls_logits\n",
    "    \n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(RegressionHead, self).__init__()\n",
    "        # Fully-connected layers for bounding box regression\n",
    "        self.fc1 = nn.Linear(in_features, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        bbox_reg = self.fc2(x)  # Bounding box regression outputs\n",
    "        return bbox_reg  # Add closing parenthesis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d369006d-ee66-429a-9136-a8fac384f071",
   "metadata": {},
   "source": [
    "c. Training:\n",
    "i.Train the faster R-CNN model on the traning dataset.\n",
    "ii.Implement a loss function that combines classification and regression losses.\n",
    "iii.Utillise data augmentation techniques such as random cropping ,fliping ,and Scalling to improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fc52f5-b3a6-451b-9ce3-deaa8588ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faster_rcnn_loss(rpn_cls_logits, rpn_reg_deltas, cls_logits, bbox_reg, targets):\n",
    "  # ... Calculate classification loss (e.g., cross-entropy loss)\n",
    "  cls_loss = ...\n",
    "\n",
    "  # ... Calculate regression loss (e.g., Smooth L1 loss)\n",
    "  reg_loss = ...\n",
    "\n",
    "  # Combine losses\n",
    "  total_loss = cls_loss + reg_loss\n",
    "  return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d14c2-52fe-48e7-a314-b24b7aec5ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f159fa-4924-48dd-9aa1-10681867b946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf76c93-aba2-4a69-83e6-6cb645d6f7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d3a7a-9ecc-40c4-b86b-7e461c0031f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36f9e0-2322-4724-9cd4-c3993f45d74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613d493-dbd8-4492-a5a3-173c74772ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7a01d-b403-477f-8f14-08b875b12b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f158c-70d3-40dc-afcb-c77fb30d6c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8936a4-bc9b-4764-b0ad-80e700dad088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe614a-badd-4fd4-99ed-3f18c142ddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b556df0-6f0c-4759-bf0e-20702c4d3963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a2960-585b-4842-8e5b-70d1792d6d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b362496-24e7-4942-8257-1f95272cc8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
